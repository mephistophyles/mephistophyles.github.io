---
layout: post
title: Staying Up-To-Date
blog_group: blog
---

I've read "when you've had to answer a question multiple times, write a blog post about it". As I'll detail more later in this post, one of the places I like to visit to keep my skills up to date and learn new things is reddit, in particular the /r/learnmachinelearning and /r/learnprogramming subreddits. A common question there is how to stay up to date with developments in such a fast moving field, which I think is directly related to another common question; how do you get up and running in the field.

There is no single right answer, especially with so many people coming from so many different backgrounds and having such diverse goals, but here I would like to outline how I stay current. I purposefully won't categorize my different resources into data science, machine learning, computer science, and other fields because I think there is enough overlap that a useful technique in one can always be used in another.

### Newsletters

There are a number of newsletters I subscribe to, they provide a fair bit of overlap which I like since it ensures that if any article is deemed important by a number of people there is a smaller chance I'll gloss over it. A helpful tip for dealing with spam/filtering newsletters can be found [here](/2018/01/01/tips).

 - [Data Elixir](https://dataelixir.com/)
 - [Data Science Weekly](https://www.datascienceweekly.org/)
 - [Data Machina](https://www.getrevue.co/profile/datamachina)
 - [O'Reilly Data Newsletter](http://www.oreilly.com/data/newsletter.html)
 - [MIT Technology Review](https://www.technologyreview.com/)
 - [Data Science Roundup](http://roundup.fishtownanalytics.com/)
 - [O'Reilly four short links Newsletter](https://www.oreilly.com/feed/four-short-links)
 - [O'Reilly Artificial Intelligence Newsletter](http://www.oreilly.com/ai/newsletter.html)
 - [Import AI](https://jack-clark.net/)
 - [Hacker Newsletter](http://www.hackernewsletter.com/)
 - [The Wild Week in AI](http://www.wildml.com/newsletter/)
 - [Kaggle Newsletter](http://blog.kaggle.com/)

That's a lot of emails every week, but I think it's great to have so many resources to choose from and even if you don't read every article and do every tutorial (which I don't), knowing it's out there is often enough. Then when it comes up in a project you're working on or in another article you can easily say "Oh I remember seeing something about that" and go look it up.

### Blogs

A lot of people in the field have written, or currently write, a blog. Since the current blogs will eventually cross your path with the above newsletters, I will only highlight a few inactive/sporadic ones that I think are worth your time. 

 - [Karpathy's old blog](http://karpathy.github.io/), in particular his famous post on [unreasonably effective RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), Karpathy is now on [Medium](https://medium.com/@karpathy/).
 - [Christopher Olah's blog](http://colah.github.io/), he explains a variety of neural network topics VERY well. He has since moved to blogging at [Distill](http://distill.pub).
 - [Count Bayesie](https://www.countbayesie.com/), a blog where statistics is used to explain interesting things (and in turn you get some better intuition for the statistics).
 - [Jake VanderPlas's blog](http://jakevdp.github.io/) will probably show up in your newsletters, but his earlier posts are also truly fascinating. 
 - [Adam Geitgey](https://medium.com/@ageitgey/) is still active, but his old series "Machine Learning is Fun" is worth a read.
 - [Chris Albon's blog](https://chrisalbon.com/) is great, but his Machine Learning/Artificial Intelligence notes are golden! 

### Tutorials

Not every how-to is written on a blog, some are written at dedicated tutorial sites (think [Datacamp](http://datacamp.com) or [Kaggle](http://kaggle.com)), and these can also be very useful to go over. Ignore the marketing promises of being turned into a data science guru in 24 hours, instead focus on learning the techniques. I very emphatically recommend just googling (or is it "Googling"?) "method-or-problem tutorial", where you fill in the method or problem type you are interested in. You can learn a lot by seeing how other people solved similar problems.

### YouTube

I know YouTube is full of people showing you the way they solve different machine learning and data science problems (e.g. [Sentdex](https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ0) and [Siraj Raval](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A)), but I personally don't use them unless there is no text-based method out there. There are two exceptions for when I do use videos for learning and they are in the next two categories.

### Conferences

Keep up with conferences around the world. That doesn't mean you need to go visit them, though the networking and discussions are certainly worth it! Most conferences post their proceedings online and some even have their presentations on YouTube. 

 - [NIPS](https://papers.nips.cc/) are online, I remember 2016's workshop on Generative Adversarial Networks to be really interesting and it generated a lot of buzz.
 - [PyData](https://www.youtube.com/user/PyDataTV) has a host of presentations and workshops on YouTube, you can easily search through them to see the interesting ones.
 - [ICML](http://techtalks.tv/icml/2016/) the international conference on machine learning hosted 2016 talks on a site called techtalks.tv (spoiler: it is still in Beta).

There are many more, which you will undoubtedly see pop-up in the newsletters. You can often look up past talks with a bit of googling. If you have access to IEEE or Springer (via a university subscription usually) you can search there and find almost all past proceedings and papers. They are generally well-archived, but if you don't have access through academia it will often cost you a pretty penny for access.

### Courses

Whether you studied computer science, statistics or something totally unrelated (I did my BSc in mechanical engineering), there is an absolute trove of information to be found online. Don't be too proud to use these resources to either brush up on some basics or gain new skills. A few courses I can highly recommend are:

 - [CS231n](http://cs231n.stanford.edu) is the (now famous) course at Stanford about Convolutional Neural Networks for Visual Recognition.
 - [CS229](http://cs229.stanford.edu) is the Stanford version of the famous Coursera machine learning course. 
 - [Stanford's Statistical Learning](https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/) is a great course that covers a subset of CS229, from a statistical angle.
 - [MIT's OCW](https://ocw.mit.edu/index.htm) is the opencourseware site from MIT. They have A LOT on there. TIP: courses that are postfixed with IAP are shorter, concentrated blocks of a specific topic.

There are more, I very much enjoyed [Stanford's Modern Algorithmic Toolbox](web.stanford.edu/class/cs168/index.html), but these are great to give you an idea of how/where to get started. There are courses available on the Coursera, Udacity or EdX platforms, but I found them to generally be a more diluted version of a true university course. 

### Conclusion

As you can see staying up to date means almost constantly being informed of new updates and acting on them. I spend several hours every week just staying up to date. Luckily I have always had the opportunity to be able to spend some of my time at work doing so, since the newest techniques are sometimes worth implementing or trying out right away. 

Don't be afraid of being overwhelmed by trying to drink from this metaphorical firehose of information, that's normal. As you get a handle on what information you need and are interested in, you become better at filtering out what you don't need to pay attention to. And in some cases, when you need to focus on other things and you develop a backlog of newsletters you can heighten your scrutiny level for when something is worth reading, or just delete a week's worth of newsletters. Staying up to date is a good habit to have, but missing a week here or a few articles there will not hurt you in the long run.
